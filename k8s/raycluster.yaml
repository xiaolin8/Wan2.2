apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: wan22-ray-cluster
spec:
  # Ray version should match the version installed in your Docker image
  rayVersion: '2.9.3'

  # -------------------------------------------------------------------
  # Ray Head Pod Configuration
  # -------------------------------------------------------------------
  headGroupSpec:
    rayStartParams:
      dashboard-host: '0.0.0.0'
    template:
      spec:
        serviceAccountName: ray-head-service-account # Example, ensure this SA exists and has permissions to create Pods, Services etc.
        containers:
          - name: ray-head
            image: 172.31.0.182/system_containers/wan22-ray:1026 # <-- Your new Ray-ready image
            ports:
              - containerPort: 6379 # Ray head internal port
                name: gcs-server
              - containerPort: 8265
                name: dashboard
              - containerPort: 10001
                name: client
            resources:
              requests:
                cpu: "2"
                memory: "4Gi"
              limits:
                cpu: "4"
                memory: "8Gi"

  # -------------------------------------------------------------------
  # Ray Worker Pods Configuration
  # This is the GPU-powered worker pool that will be autoscaled.
  # -------------------------------------------------------------------
  workerGroupSpecs:
    - groupName: gpu-workers
      # This is the key for autoscaling. 
      # We start with 0 replicas, KEDA will scale this up.
      replicas: 0
      minReplicas: 0
      maxReplicas: 4 # Example max, adjust based on your cluster capacity
      rayStartParams: {}
      template:
        spec:
          containers:
            - name: ray-worker
              image: 172.31.0.182/system_containers/wan22-ray:1026
              lifecycle:
                preStop:
                  exec:
                    command: ["/bin/sh","-c","ray stop"]
              resources:
                requests:
                  cpu: "4"
                limits:
                  cpu: "64"
                  memory: 256Gi
                  nvidia.com/gpu: "2"
                  t7d.com/rdma: "1"
              # Task-specific variables like WAN_FRAME_NUM are omitted as they will be passed dynamically
              env:
                - name: NVIDIA_VISIBLE_DEVICES
                  value: "all"
                - name: NCCL_IB_DISABLE
                  value: "0"
                - name: NCCL_SOCKET_IFNAME
                  value: "^lo,docker0,veth"
                - name: AWS_ACCESS_KEY_ID
                  valueFrom:
                    secretKeyRef:
                      name: s3-credentials
                      key: S3_ACCESS_KEY
                - name: AWS_SECRET_ACCESS_KEY
                  valueFrom:
                    secretKeyRef:
                      name: s3-credentials
                      key: S3_SECRET_KEY
              # The initContainer-related 'image-volume' is omitted
              volumeMounts:
                - name: data-volume
                  mountPath: /data/Wan2.2-I2V-A14B
                - name: output-volume
                  mountPath: /output
                - name: dshm
                  mountPath: /dev/shm
          volumes:
            - name: data-volume
              hostPath:
                path: /data/Wan-AI/Wan2.2-I2V-A14B
            - name: output-volume
              hostPath:
                path: /data/Wan-AI/output
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: 128Gi