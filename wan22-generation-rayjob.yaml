apiVersion: ray.io/v1
kind: RayJob
metadata:
  name: wan22-generation-rayjob
spec:
  entrypoint: python generate_ray.py --task \"t2v-A14B\" --size \"832*480\" --ckpt_dir /models/Wan2.2-T2V-A14B --ulysses_size 4 --prompt \"A 15-second cinematic video of an astronaut walking on Mars and discovering a giant stone engraved with 'TenxCloud'.\" --save_file /workspace/outputs/$(date '+%Y%m%d%H%M%S')-tenxcloud-from-ray.mp4 --num-workers 4 --sample_guide_scale 10 --convert_model_dtype --offload_model True --t5_cpu --dit_fsdp
  # 任务完成后，自动删除 Ray 集群
  shutdownAfterJobFinishes: true
  rayClusterSpec:
    rayVersion: '2.10.0' # 最好指定 Ray 的版本
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
      template:
        spec:
          containers:
            - name: ray-head
              image: 172.31.0.182/system_containers/wan22:1016-ray
              imagePullPolicy: Always
              env:
                - name: PYTORCH_CUDA_ALLOC_CONF
                  value: expandable_segments:True
              ports:
                - name: gcs-server
                  containerPort: 6379
                - name: dashboard
                  containerPort: 8265
                - name: client
                  containerPort: 10001
              resources:
                limits:
                  memory: "256Gi"
                requests:
                  memory: "16Gi"
              volumeMounts:
                - name: model-storage
                  mountPath: /models
                - name: output-storage
                  mountPath: /workspace/outputs
          volumes:
            - name: model-storage
              hostPath:
                path: /data/Wan-AI
            - name: output-storage
              hostPath:
                path: /data/Wan-AI/output
    workerGroupSpecs:
      - replicas: 2
        minReplicas: 2
        maxReplicas: 2
        groupName: gpu-worker-group
        rayStartParams: {}
        template:
          spec:
            affinity:
              podAntiAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  - labelSelector:
                      matchExpressions:
                        - key: ray.io/group
                          operator: In
                          values:
                            - gpu-worker-group
                    topologyKey: kubernetes.io/hostname
            tolerations:
              - key: "nvidia.com/gpu"
                operator: "Exists"
                effect: "NoSchedule"
            containers:
              - name: ray-worker
                image: 172.31.0.182/system_containers/wan22:1016-ray
                imagePullPolicy: Always
                env:
                  - name: PYTORCH_CUDA_ALLOC_CONF
                    value: expandable_segments:True
                resources:
                  limits:
                    memory: 128Gi
                    nvidia.com/gpu: "2"
                  requests:
                    memory: 128Gi
                    nvidia.com/gpu: "2"
                securityContext:
                  capabilities:
                    add:
                    - IPC_LOCK
                volumeMounts:
                  - name: model-storage
                    mountPath: /models
                  - name: output-storage
                    mountPath: /workspace/outputs
            volumes:
              - name: model-storage
                hostPath:
                  path: /data/Wan-AI
              - name: output-storage
                hostPath:
                  path: /data/Wan-AI/output
