apiVersion: ray.io/v1
kind: RayCluster
metadata:
  name: wan22-ray-cluster
  annotations:
    ray.io/worker-group-idle-timeout-seconds: "600"
spec:
  rayVersion: '2.10.0'
  enableInTreeAutoscaling: true
  headGroupSpec:
    serviceType: NodePort
    rayStartParams:
      dashboard-host: '0.0.0.0'
    template:
      spec:
        hostPID: true
#        affinity:
#          podAntiAffinity:
#            requiredDuringSchedulingIgnoredDuringExecution:
#              - labelSelector:
#                  matchExpressions:
#                    - key: ray.io/group
#                      operator: In
#                      values:
#                        - gpu-worker-group
#                topologyKey: kubernetes.io/hostname
        containers:
          - name: ray-head
            image: 172.31.0.182/system_containers/wan22:1016-ray
            imagePullPolicy: Always
            env:
              - name: PYTORCH_CUDA_ALLOC_CONF
                value: expandable_segments:True
              - name: CUDA_DISABLE_CONTROL
                value: "true"
            ports:
              - name: gcs-server
                containerPort: 6379
              - name: dashboard
                containerPort: 8265
              - name: client
                containerPort: 10001
            resources:
              limits:
                cpu: "16"
                memory: "256Gi"
                nvidia.com/gpu: "2"
                nvidia.com/priority: "0"
              requests:
                cpu: "2"
                memory: "4Gi"
                nvidia.com/gpu: "2"
                nvidia.com/priority: "0"
            volumeMounts:
              - name: model-storage
                mountPath: /models
              - name: output-storage
                mountPath: /workspace/outputs
        tolerations:
          - key: "nvidia.com/gpu"
            operator: "Exists"
            effect: "NoSchedule"
        volumes:
          - name: model-storage
            hostPath:
              path: /data/Wan-AI
          - name: output-storage
            hostPath:
              path: /data/Wan-AI/output
  workerGroupSpecs:
    - replicas: 0
      minReplicas: 0
      maxReplicas: 3
      groupName: gpu-worker-group
      rayStartParams: { }
      template:
        spec:
          hostPID: true
#          affinity:
#            podAntiAffinity:
#              requiredDuringSchedulingIgnoredDuringExecution:
#                - labelSelector:
#                    matchExpressions:
#                      - key: ray.io/group
#                        operator: In
#                        values:
#                          - gpu-worker-group
#                  topologyKey: kubernetes.io/hostname
          tolerations:
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
          containers:
            - name: ray-worker
              image: 172.31.0.182/system_containers/wan22:1016-ray
              imagePullPolicy: Always
              env:
                - name: PYTORCH_CUDA_ALLOC_CONF
                  value: expandable_segments:True
                - name: CUDA_DISABLE_CONTROL
                  value: "true"
              resources:
                limits:
                  cpu: "16"
                  memory: "256Gi"
                  nvidia.com/gpu: "2"
                  nvidia.com/priority: "0"
                requests:
                  cpu: "2"
                  memory: "4Gi"
                  nvidia.com/gpu: "2"
                  nvidia.com/priority: "0"
              securityContext:
                capabilities:
                  add:
                    - IPC_LOCK
              volumeMounts:
                - name: model-storage
                  mountPath: /models
                - name: output-storage
                  mountPath: /workspace/outputs
          volumes:
            - name: model-storage
              hostPath:
                path: /data/Wan-AI
            - name: output-storage
              hostPath:
                path: /data/Wan-AI/output
