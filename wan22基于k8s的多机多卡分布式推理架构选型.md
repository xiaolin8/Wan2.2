# 《wan22 基于 Kubernetes 的多机多卡分布式推理架构选型》

## 1. 概述

本文档旨在为 `wan2.2` 视频生成体验中心项目，提供一套基于 Kubernetes (K8s) 的多机多卡分布式推理架构选型方案。核心目标是将 `wan2.2` 的视频生成能力从一次性的批处理作业，转化为一个用户友好的、可交互的在线服务，同时兼顾高性能、高可用、高可扩展性及成本效益。

## 2. 核心需求与挑战

1.  **多机多卡分布式推理**: `wan2.2` 模型（特别是 14B 级别）需要利用多台机器上的多块 GPU 进行分布式推理，以满足计算性能要求。
2.  **长耗时、资源密集型任务**: 视频生成是计算密集型任务，单次推理可能需要数分钟，且占用大量 GPU 显存。
3.  **用户交互体验**: 用户在体验中心页面上发起请求后，需要能够实时追踪任务状态，并在完成后获取结果，而非长时间等待 HTTP 响应。
4.  **成本效益**: GPU 资源昂贵，需避免在无请求时“空烧”资源，实现按需付费或智能缩容。
5.  **Go 技术栈偏好**: 后端 API 和任务队列倾向于使用 Go 语言生态。
6.  **复用现有 PyTorch 分布式逻辑**: 希望最大化复用 `generate.py` 中已有的 `torch.distributed`、FSDP 等分布式推理逻辑。

## 3. 架构模式选择

针对长耗时、资源密集型任务，主要有两种架构模式：

### 3.1 模式一：作业模式 (Job-based Inference)

*   **描述**: 将每次推理请求视为一个独立的、一次性的批处理作业。当请求到来时，启动一个 K8s Job（如 `PyTorchJob`），任务完成后 Job 自动终止，释放资源。
*   **优点**: 成本控制完美（按需付费），资源利用率高，任务间隔离性强。
*   **缺点**: 每次任务都有较高的启动延迟（Pod 调度、镜像拉取、模型加载），不适合对延迟敏感的场景。
*   **适用场景**: 离线批处理、异步任务、对延迟容忍度高的场景。

### 3.2 模式二：长驻服务模式 (Resident Service Inference)

*   **描述**: 将模型部署为一个持续运行的在线服务。模型在服务启动时加载一次，之后持续监听 HTTP 请求，并对请求进行推理。
*   **优点**: 模型处于“热”状态，推理延迟低，用户体验好。
*   **缺点**: 持续占用资源（特别是 GPU），成本高昂。若无智能缩容，资源浪费严重。
*   **适用场景**: 低延迟、高频次、短耗时的在线推理。

### 3.3 模式三：智能长驻服务模式 (Hybrid - Resident Service with Scale-to-Zero)

*   **描述**: 结合了模式二的低延迟和模式一的成本效益。模型部署为长驻服务，但通过智能的自动缩容机制（特别是缩容到零），在无请求时释放资源，有请求时快速扩容。
*   **优点**: 兼顾低延迟和成本效益，提供最佳的用户体验和资源管理。
*   **缺点**: 架构复杂，引入额外的平台组件，冷启动时仍有延迟。
*   **适用场景**: 对延迟有要求，但请求量有波峰波谷的在线推理服务。

## 4. 详细方案对比与选型

### 4.1 方案一：基于 PyTorchJob 的作业模式 (当前方案)

*   **核心思路**: 保持现有架构，Go 后端 API 接收请求后，将任务推送到异步队列。Worker 从队列中取出任务，然后通过 `client-go` 创建一个 `PyTorchJob`。`PyTorchJob` 负责启动多机多卡分布式推理，完成后将结果上传 S3。
*   **多机多卡实现**: `PyTorchJob` 天然支持多机多卡。`torchrun` 会在多个 Pod 之间协调 `torch.distributed` 进程组。
*   **优点**: 
    *   **成本效益极高**: GPU 资源按需使用，任务完成后立即释放。
    *   **架构简单**: 你已熟悉 `PyTorchJob` 的使用。
    *   **鲁棒性强**: 任务间隔离，一个任务失败不影响其他。
    *   **复用度高**: `generate.py` 可直接作为 `PyTorchJob` 的入口脚本。
*   **缺点**: 
    *   **用户体验有延迟**: 每次任务都有 Pod 调度、镜像拉取、模型加载等冷启动时间，用户需等待数分钟。
*   **适用场景**: **初期快速上线、成本敏感、用户对视频生成有一定延迟容忍度的体验中心。**

### 4.2 方案二：基于 KServe + TorchServe 的长驻服务模式

*   **核心思路**: 利用 KServe 平台管理模型服务的生命周期和流量，底层推理引擎使用 PyTorch 官方的 TorchServe。Go 后端 API 直接调用 KServe 暴露的模型服务接口。
*   **多机多卡实现**: 
    *   KServe 会部署一个 `StatefulSet` 来承载 TorchServe 服务，确保每个 Pod 具有稳定的网络标识。
    *   TorchServe 的 `handler.py` 需要被设计为分布式应用，在 `initialize` 方法中调用 `torch.distributed.init_process_group()`，并加载 FSDP 模型。
    *   KServe 运行时会负责注入 `MASTER_ADDR`, `MASTER_PORT`, `RANK`, `WORLD_SIZE` 等环境变量，使 TorchServe 的多个 Pod 能够形成一个分布式组。
    *   KServe 的路由层会将请求发送到分布式组的“主”Pod，由其协调推理。
*   **优点**: 
    *   **平台级管理**: 获得 KServe 的自动缩容到零、流量管理、版本控制等所有优势。
    *   **标准化**: 使用 PyTorch 官方推荐的模型服务方式。
    *   **低延迟**: 模型常驻内存，一旦服务“热”起来，推理延迟极低。
    *   **成本控制**: 自动缩容到零，有效控制 GPU 成本。
*   **缺点**: 
    *   **配置复杂**: 多机多卡的 TorchServe 在 KServe 中的配置相对复杂，需要深入理解 `torch.distributed` 和 KServe 的高级配置（如 `StatefulSet`、环境变量注入）。
    *   **冷启动延迟**: 从零扩容时，整个分布式组的初始化和模型加载仍需时间。
    *   **重构**: `generate.py` 需重构为 TorchServe 的 `handler.py` 格式。
*   **适用场景**: **对用户体验的实时性要求高、请求量有波峰波谷、愿意投入更多精力进行平台建设的场景。**

### 4.3 方案三：基于 Ray Serve 的长驻服务模式

*   **核心思路**: 利用 Ray Cluster 的原生分布式能力和 Ray Serve 的模型服务框架。Go 后端 API 直接调用 Ray Serve 暴露的模型服务接口。
*   **多机多卡实现**: 
    *   部署一个多节点的 Ray Cluster（通过 KubeRay Operator），它本身就管理着多机多卡的资源。
    *   Ray Serve 将模型部署为多个 Ray Actor，每个 Actor 可以请求 GPU。Ray 会自动将这些 Actor 分布到 Ray Cluster 的不同节点和 GPU 上。
    *   你可以在 Ray Actor 内部使用 `torch.distributed` 或 Ray 的分布式原语来管理模型副本和数据分发。
    *   Ray Serve 提供了 `placement_group` 等功能，确保分布式组件的协同。
*   **优点**: 
    *   **原生分布式**: Ray 的设计理念就是分布式，多机多卡配置更自然、灵活。
    *   **高度灵活**: 可以精细控制模型的部署拓扑、资源分配和请求处理逻辑。
    *   **统一生态**: 如果未来有训练、数据处理等其他分布式任务，Ray 提供统一平台。
    *   **低延迟与成本控制**: Ray Serve 提供了自动扩缩容能力，实现低延迟和成本效益。
*   **缺点**: 
    *   需要学习 Ray 和 Ray Serve 的编程模型和部署方式。
    *   Ray Cluster 的部署和管理需要一定经验。
    *   重构: `generate.py` 需重构为 Ray Serve 的 Actor 格式。
*   **适用场景**: **对分布式模型服务有高度定制化需求、希望在统一平台管理所有 ML 工作流、或已熟悉 Ray 生态的场景。**

## 5. 推荐的后端技术栈 (Go-based)

无论选择哪种架构模式，Go 后端技术栈的组件保持一致：

*   **后端 API 服务**: **Gin** (高性能、轻量级 Go Web 框架)
*   **异步任务队列**: **Asynq** (Go 语言的分布式任务队列库) + **Redis** (作为消息中间件)
*   **Kubernetes 交互**: **client-go** (K8s 官方 Go 客户端库)

## 6. 最终架构选型建议

综合考虑您的需求、挑战和技术栈偏好，给出以下建议：

1.  **短期/初期建议：采用“基于 PyTorchJob 的作业模式”**
    *   **理由**: 这是最快、最简单、最经济的上线方案。你已经拥有 `PyTorchJob` 的 YAML 和 `generate.py`。只需构建 Go API 和 Asynq 任务队列，即可实现用户交互和异步处理。虽然每次有冷启动，但对于视频生成这种任务，用户有一定延迟容忍度。这能让你快速验证业务价值，并控制初期成本。

2.  **长期/演进建议：当业务需求驱动时，考虑“智能长驻服务模式”**
    *   **驱动因素**: 当用户量和请求频率显著增加，且用户对每次生成任务的**热启动延迟**（即模型已加载后的推理延迟）有极高要求时，再考虑向长驻服务模式演进。
    *   **推荐方案**: 
        *   **Ray Serve**: 如果你希望在分布式模型服务方面获得最大的灵活性和原生支持，并且愿意投入学习 Ray 生态。Ray Serve 在多机多卡分布式模型服务方面表现出色。
        *   **KServe + TorchServe**: 如果你更倾向于 K8s 原生平台，并希望利用 PyTorch 官方的工具链。但请注意，多机多卡的配置会相对复杂，需要深入理解 KServe 的高级配置。

**核心思想**: 从最简单、最经济的方案开始，根据业务发展和实际痛点逐步演进。避免过度设计，但要为未来的演进留好接口和可能性。
