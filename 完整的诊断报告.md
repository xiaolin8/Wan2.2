# Hami vGPU调度模式下FSDP分布式任务性能瓶颈诊断报告

## 1. 摘要

在使用Hami vGPU调度器的Kubernetes环境中，运行基于PyTorch FSDP的分布式应用时遇到的灾难性性能瓶颈问题。

经过详尽排查，我们**100%确认问题的根源在于Hami的vGPU实现未能将节点内GPU之间的NVLink物理拓扑信息透传给容器**。这导致PyTorch的分布式通信库NCCL降级使用慢速的TCP/IP网络进行通信，由此产生的巨大通信开销完全抵消了FSDP使用多GPU带来的性能增益，并使得应用无法在生产环境中使用。

## 2. 问题现象

- **应用类型**: 一个使用PyTorch FSDP (Fully Sharded Data Parallelism)策略的文生视频（Text-to-Video）分布式推理任务。
- **基准性能**: 在一台本地的、拥有8块物理GPU的服务器上，完成一次推理任务耗时约**6分钟**。
- **观测性能**: 在使用Hami vGPU的Kubernetes集群上，无论使用1个vGPU还是4个vGPU，完成相同的推理任务耗时均在**25-30分钟**左右。性能没有随GPU数量增加而提升，且远慢于基准性能。

## 3. 集群环境

- **物理GPU**: 8 x NVIDIA H20
- **NVIDIA驱动**: (通过`nvidia-smi`确认在宿主机上工作正常)
- **容器运行时**: Docker 28.1.1
- **GPU调度方案**: Hami Scheduler (相关组件部署在`system-gpu`命名空间)

## 4. 排查过程与证据

我们进行了一系列逐层深入的排查，以排除其他可能性。

### 4.1. 磁盘I/O瓶颈排查

- **假设**: 模型加载缓慢（启动耗时2-3分钟）是由于慢速磁盘导致。
- **证据**: 我们在节点`/data`目录（容器的HostPath挂载源）上执行了`dd`读写测试。
- **结果**: 磁盘顺序读取速度高达 **4.2 GB/s**。
- **结论**: **磁盘I/O不是瓶颈。**

### 4.2. K8s资源分配瓶颈排查

- **假设**: Pod的CPU `requests`过低导致计算时被节流。
- **证据**: 我们将Worker Pod的CPU `requests`和`limits`均设置为`8`，确保其获得`Guaranteed`的QoS等级。
- **结果**: 任务耗时没有改善。
- **结论**: **K8s的CPU资源分配不是瓶颈。**

### 4.3. 应用层配置瓶颈排查

- **假设**: 应用启动参数`--t5_cpu`强制将T5模型放在CPU上运行，导致性能问题。
- **证据**: 我们编辑了`RayJob`的启动命令，移除了`--t5_cpu`参数，让T5模型在GPU上运行。
- **结果**: 任务耗时几乎没有变化，依然在25-30分钟范围。
- **结论**: **T5模型的位置不是主要瓶颈。**

### 4.4. 分布式通信效率排查

- **假设**: FSDP的通信开销是主要瓶颈。
- **证据1**: 我们将任务设置为仅使用1个GPU运行（此时无分布式通信）。任务耗时依然在25-30分钟左右。这证明了**“4个GPU的运行速度 ≈ 1个GPU的运行速度”**，说明增加的3个GPU带来的算力，完全被分布式通信的开销所吞噬。
- **证据2**: 在最后的测试中，我们为容器注入`NCCL_DEBUG=INFO`环境变量后，分布式任务在初始化时直接报错，日志显示`Timeout waiting for key`。这证明了分布式进程之间在10分钟内都无法完成基础的“握手”通信。
- **结论**: **分布式通信存在灾难性的瓶颈。**

### 4.5. Hami调度器与物理资源排查

- **假设**: 通信瓶颈是Hami vGPU未能透传NVLink信息导致。
- **证据1 (硬件完好)**: 在宿主机`bjdb-h20-node-082`上执行`nvidia-smi nvlink -s`，成功输出了健康的NVLink矩阵。
- **证据2 (Hami独占调度)**: 我们尝试部署一个申请标准物理GPU资源（`nvidia.com/gpu: 1`）的测试Pod，该Pod状态长时间处于`Pending`。同时，`describe node`显示该节点上报的GPU资源是`nvidia.com/gpu: 80`（8块物理卡 x 10倍虚拟化）。这证明了Hami已完全接管GPU调度，标准方式无法申请到物理GPU。
- **结论**: **Hami强制所有GPU任务通过其vGPU方案运行，但其方案未能将容器外部健康的NVLink拓扑暴露给容器内部。**

## 5. 最终的决定性证据

为了最终证实Hami是导致问题的根源，我们部署了一个申请标准物理GPU（`nvidia.com/gpu: 1`）的测试Pod `nvlink-test-pod`。我们观察到以下关键现象：

1.  **Pod被Hami拦截**: `describe pod`信息显示，这个Pod被`hami-scheduler`成功调度，并且包含了Hami注入的注解，如`hami.io/vgpu-devices-allocated`。这证明了Hami强制接管了所有`nvidia.com/gpu`资源的调度。

2.  **NVLink拓扑丢失**: `kubectl logs nvlink-test-pod`的日志（即容器内`nvidia-smi nvlink -s`的输出）**只显示了单张GPU的信息**，而不是像在宿主机上那样显示出8块GPU互相连接的完整矩阵。

**这份日志无可辩驳地证明了：Hami的vGPU容器环境，未能将节点内完整的NVLink物理拓扑透传给容器。这是导致NCCL降级到慢速TCP/IP网络的直接原因。**

## 6. 根本原因总结

**Hami的vGPU实现，在为容器提供CUDA算力的同时，丢失了物理GPU之间底层的NVLink拓扑信息。这使得容器内的NCCL库无法利用高速的NVLink总线进行通信，被迫降级到基于TCP/IP的慢速容器网络。对于FSDP这类通信密集型的分布式策略，这种降级是致命的，其产生的巨大通信延迟是导致应用性能崩溃的唯一原因。**

## 7. 建议解决方案

我们请求Hami团队审阅以上证据，并提供支持。我们建议的解决方案是：

1.  **修复Hami（治本）**: 升级或修复Hami的实现，使其在分配vGPU时，能够完整地将NVLink拓扑信息透传给容器，让NCCL能够正常工作。
2.  **提供绕行机制（治标）**: 提供一种方式，让需要高性能通信的任务可以选择性地绕过Hami的vGPU调度，直接获取物理GPU的分配。

为了方便Hami团队复现此问题，我们提供以下测试Pod的YAML。在当前环境中，此Pod会被Hami调度并暴露出NVLink信息丢失的问题。

```yaml
# nvlink-test-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nvlink-test-pod
  namespace: hu # 请根据实际情况修改namespace
spec:
  restartPolicy: Never
  containers:
  - name: test
    image: nvidia/cuda:11.8.0-base-ubuntu22.04
    command: ["/bin/sh", "-c", "nvidia-smi nvlink -s"]
    resources:
      limits:
        nvidia.com/gpu: 1 # 申请一块标准的物理GPU
  tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
```
